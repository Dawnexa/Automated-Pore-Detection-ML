{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model development \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation Installation:\n",
    "\n",
    "The needed packages are listed in the [README](README.md) file. It also contains how to install and how to use them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to the p4p-Project Notebook written by David, Michael and Alina. In this Notebook we will cover a few things:\n",
    "\n",
    "- Structure of our neural network\n",
    "- looking at the input data \n",
    "- training and testing of the neural network - accompanied by its visualization. \n",
    "- looking at the output data in comparison to the input\n",
    "- processing of the output by displaying it in a histogram - accompanied by visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of our neural network\n",
    "\n",
    "Let's take a look at a picture of our neural network:\n",
    "<p align=\"center\">\n",
    "    <img src=\"notebooks/notebook_stuff/Unet_Structure.jpeg\" width=\"1000\" height=\"585\">\n",
    "</p>\n",
    "\n",
    "As we can see, it takes an input image, performs a convolution (and a ReLu) on it and thus gives the picture more \"depth\".\n",
    "Then we use `max pool 2x2` to get smaller pictures where the important features are retained, we then do another convolution (and ReLu) on it, and we repeat that process until we reach the \"bottleneck\". We then use `up-conv 2x2` to \"up-scale\" the pictures again to the desired size. The `final_conv` is a convolutional layer in the U-Net architecture that reduces the number of feature channels to the desired number of output channels, effectively mapping the extracted features to the predicted output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data\n",
    "\n",
    "Lets take a look at the input-data, and compare them to the mask that we want to have:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"notebooks/notebook_stuff/org1.png\" width=\"365\" height=\"365\">\n",
    "  <img src=\"notebooks/notebook_stuff/mask1.png\" width=\"365\" height=\"365\">\n",
    "</p>\n",
    "\n",
    "The segmentation was done by editing tools to get the first training and test data that we can use.\n",
    "The dataset is processed by functions created in [dataset.py](dataset.py) and [utils.py](utils.py) (dataset.py in particular is a class)\n",
    "\n",
    "Lets get straight to the training loop:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing loop\n",
    "\n",
    "The training loop is implemented in [train.py](train.py) by the `train_fn()` and the `valid_fn` functions (see the code for further documentation and comments)\n",
    "\n",
    "Another function in the utils.py file called `check_accuracy()`` checks the accuracy by comparing the model's predictions with the actual values. It does this by iterating over each batch of data in the provided data loader, making predictions with the model, and then comparing these predictions to the actual values. The function counts the number of correct predictions and the total number of predictions to calculate the accuracy. The function returns the accuracy as a percentage.\n",
    "\n",
    "This is a plot for the model's accuracy during the training process:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"output/plot.png\" width=\"640\" height=\"480\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training, we use the [predict.py](src/predict.py) \"function\" to print us some comparable results of the model's mask and the desired one. Let's take a look at 5 examples:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"notebooks/notebook_stuff/preds/344.png\" width=\"500\" height=\"500\">\n",
    "  <img src=\"notebooks/notebook_stuff/preds/345.png\" width=\"500\" height=\"500\">\n",
    "  <img src=\"notebooks/notebook_stuff/preds/347.png\" width=\"500\" height=\"500\">\n",
    "  <img src=\"notebooks/notebook_stuff/preds/348.png\" width=\"500\" height=\"500\">\n",
    "  <img src=\"notebooks/notebook_stuff/preds/394.png\" width=\"500\" height=\"500\">\n",
    "</p>\n",
    "\n",
    "\n",
    "As we can see, the model works pretty accurate (actually 98,78% accuracy is achieved, which is amazing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the output of the neural network (visualizing it) and creating a histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot is received using the modified depth-first-search method which is part of the Class Material. It can be found in the Notebook called [DFS Visualization](notebooks/DFS Visualization.ipynb).\n",
    "For each entry of self.matrix that has been examined by the dfs a point is being displayed in the plot. The green area represents the cavity. It is sorrunded by a layer of red points. These show that the dfs has reached a dead end once it reached 'False' in self.matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align = \"center\">\n",
    "    <img src=\"notebooks/notebook_stuff/dfs_visualization.png\" height=\"393.6\" width=\"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram and comparing it to desired output.\n",
    "\n",
    "Let's take a look at the histogram of the data:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"notebooks/notebook_stuff/modelhistogram.png\" height=\"656\" width=\"1000\">\n",
    "</p>\n",
    "\n",
    "As we can see here, according to our model the size of the most common pore is 1 pixel per pore, with 317 pores.\n",
    "\n",
    "Let's compare that to the data if we calculate everything by hand:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"notebooks/notebook_stuff/vergleichhistogram.png\" height=\"312\" width=\"1000\">\n",
    "</p>\n",
    "\n",
    "As we can see in the plot of the training masks, the \"real\" size of the pore with the most pores is also 1, with 317 pores. We can also see that even if the model's not perfectly accurate (as expected) it still goes in the right direction. Now with more data, or a more advanced model, we could get better results, but for the time being spent into this project and the limited available data to train it on, the model is almost \"perfect\". "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
